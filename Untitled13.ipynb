{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN3iMwHtOFxnkfMN8uQOLYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haouasfares/stockcode/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCpLWle7uZRl"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -q tensorflow_text\n",
        "!pip install -q simpleneighbors[annoy]\n",
        "!pip install -q nltk\n",
        "!pip install -q tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIy3SudGugoX"
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import simpleneighbors\n",
        "import urllib\n",
        "from IPython.display import HTML, display\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_text import SentencepieceTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def download_squad(url):\n",
        "  return json.load(urllib.request.urlopen(url))\n",
        "\n",
        "def extract_sentences_from_squad_json(squad):\n",
        "  all_sentences = []\n",
        "  for data in squad['data']:\n",
        "    for paragraph in data['paragraphs']:\n",
        "      sentences = nltk.tokenize.sent_tokenize(paragraph['context'])\n",
        "      all_sentences.extend(zip(sentences, [paragraph['context']] * len(sentences)))\n",
        "  return list(set(all_sentences)) # remove duplicates\n",
        "\n",
        "def extract_questions_from_squad_json(squad):\n",
        "  questions = []\n",
        "  for data in squad['data']:\n",
        "    for paragraph in data['paragraphs']:\n",
        "      for qas in paragraph['qas']:\n",
        "        if qas['answers']:\n",
        "          questions.append((qas['question'], qas['answers'][0]['text']))\n",
        "  return list(set(questions))\n",
        "\n",
        "def output_with_highlight(text, highlight):\n",
        "  output = \"<li> \"\n",
        "  i = text.find(highlight)\n",
        "  while True:\n",
        "    if i == -1:\n",
        "      output += text\n",
        "      break\n",
        "    output += text[0:i]\n",
        "    output += '<b>'+text[i:i+len(highlight)]+'</b>'\n",
        "    text = text[i+len(highlight):]\n",
        "    i = text.find(highlight)\n",
        "  return output + \"</li>\\n\"\n",
        "\n",
        "def display_nearest_neighbors(query_text, answer_text=None):\n",
        "  query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
        "  search_results =index.nearest(query_embedding, n=num_results)\n",
        "  return search_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4kMju0ivMTT"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\"                            \n",
        "model = hub.load(module_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN6DhfG9v1kE"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "encodings = model.signatures['response_encoder'](\n",
        "  input=tf.constant([dataset['Answers']['Body'][0]]),\n",
        "  context=tf.constant([dataset['Answers']['Body'][1]]))\n",
        "index = simpleneighbors.SimpleNeighbors(\n",
        "    len(encodings['outputs'][0]), metric='angular')\n",
        "\n",
        "\n",
        "print('Computing embeddings for %s sentences' % len(dataset['Answers']['Body'][:1000]))\n",
        "slices = zip(*(iter(dataset['Answers']['Body'][:1000]),) * batch_size)\n",
        "num_batches = int(len(dataset['Answers']['Body'][:1000]) / batch_size)\n",
        "for s in tqdm(slices, total=num_batches):\n",
        "  response_batch = list([r for r in s])\n",
        "  context_batch = list([c for c in s])\n",
        "  encodings = model.signatures['response_encoder'](\n",
        "    input=tf.constant(response_batch),\n",
        "    context=tf.constant(context_batch)\n",
        "  )\n",
        "  for batch_index, batch in enumerate(response_batch):\n",
        "    index.add_one(batch, encodings['outputs'][batch_index])\n",
        "\n",
        "index.build()\n",
        "print('simpleneighbors index for %s sentences built.' % len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2gIv4GNVlMu"
      },
      "source": [
        "num_results = 1\n",
        "\n",
        "query = random.choice(dataset1['Questions']['Body'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mxB0iPc0umT"
      },
      "source": [
        "!pip install datasets\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d430okWv1e8k"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"so_stacksample\",\"Answers\", data_dir=\"/content/dowland\")\n",
        "dataset['Answers']['Body'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tI-gTu22hgC"
      },
      "source": [
        "dataset1 = load_dataset(\"so_stacksample\",\"Questions\", data_dir=\"/content/dowland\")\n",
        "dataset2 = load_dataset(\"so_stacksample\",\"Tags\", data_dir=\"/content/dowland\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzYLvNcCW-vF"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XHDCZ2gXAhR"
      },
      "source": [
        "!pip install discord\n",
        "import discord\n",
        "\n",
        "from discord.ext import commands\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgjaGFUU3KLX"
      },
      "source": [
        "client = commands.Bot(command_prefix='!')\n",
        "\n",
        "\n",
        "@client.event\n",
        "async def on_ready():\n",
        "    print('Logged on as {0.user}'.format(client))\n",
        "\n",
        "\n",
        "@client.command()\n",
        "async def test(ctx):\n",
        "    sender_name = str(ctx.message.author)\n",
        "    await ctx.send('Hello '+sender_name.split('#')[0])\n",
        "\n",
        "\n",
        "@client.command()\n",
        "async def question(ctx):\n",
        "  \n",
        "  q = str(ctx.message.content)[10:]\n",
        "  w = display_nearest_neighbors(q)\n",
        "  await ctx.send(w[0]);  \n",
        "  \n",
        "  \n",
        "\n",
        "@client.command()\n",
        "async def clear(ctx,amount=1):\n",
        "    await ctx.channel.purge(limit=amount)\n",
        "\n",
        "client.run(\"ODQ1MDYxMDQ1NTMzODAyNTM2.YKbebQ.VP8co63aZqVN2vVXUk57Ip7vRe8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWVYMgWhD5Yu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}